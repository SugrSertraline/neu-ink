"""
å¤§æ¨¡å‹å·¥å…·ç±»
æ”¯æŒå¤šç§å¤§æ¨¡å‹è°ƒç”¨ï¼Œç›®å‰é›†æˆ GLM-4.6 æ¨¡å‹
"""

import os
import json
import requests
from typing import Dict, Any, Optional, List
from enum import Enum

class LLMModel(Enum):
    """æ”¯æŒçš„å¤§æ¨¡å‹æšä¸¾"""
    GLM_4_6 = "glm-4.6"
    GLM_4_5 = "glm-4.5"
    GLM_4_PLUS = "glm-4-plus"
    # æœªæ¥å¯ä»¥æ‰©å±•å…¶ä»–æ¨¡å‹
    # GPT_4 = "gpt-4"
    # CLAUDE_3 = "claude-3"

class LLMUtils:
    """å¤§æ¨¡å‹å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.glm_api_key = os.getenv('GLM_API_KEY')
        self.glm_base_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        print(f"ğŸ”‘ GLM API Key çŠ¶æ€: {'âœ… å·²é…ç½®' if self.glm_api_key and self.glm_api_key != 'your_glm_api_key_here' else 'âŒ æœªé…ç½®æˆ–ä¸ºå ä½ç¬¦'}")
        print(f"ğŸŒ APIç«¯ç‚¹: {self.glm_base_url}")
        
    def call_llm(
        self,
        messages: List[Dict[str, str]],
        model: LLMModel = LLMModel.GLM_4_6,
        temperature: float = 1.0,
        max_tokens: int = 65536,
        stream: bool = False,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨å¤§æ¨¡å‹æ¥å£
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"role": "user", "content": "..."}]
            model: ä½¿ç”¨çš„æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            **kwargs: å…¶ä»–æ¨¡å‹ç‰¹å®šå‚æ•°
            
        Returns:
            æ¨¡å‹å“åº”ç»“æœæˆ– Noneï¼ˆå¦‚æœå‡ºé”™ï¼‰
        """
        if model == LLMModel.GLM_4_6:
            return self._call_glm(messages, temperature, max_tokens, stream, **kwargs)
        elif model == LLMModel.GLM_4_5:
            return self._call_glm(messages, temperature, max_tokens, stream, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
    
    def _call_glm(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
        stream: bool,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨ GLM æ¨¡å‹
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            GLM å“åº”ç»“æœæˆ– None
        """
        print("ğŸš€ å¼€å§‹è°ƒç”¨GLM API")
        print(f"ğŸ“ è¯·æ±‚æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        if not self.glm_api_key:
            print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® GLM_API_KEY ç¯å¢ƒå˜é‡")
            return None
            
        # ä½¿ç”¨æ›´æ–°çš„æ¨¡å‹åç§°
        payload = {
            "model": LLMModel.GLM_4_6.value,  # ä¿®å¤ï¼šä½¿ç”¨ glm-4.6
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
            **kwargs
        }
        
        print(f"ğŸ¤– æ¨¡å‹: {payload['model']}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {payload['temperature']}")
        print(f"ğŸ“ æœ€å¤§Tokenæ•°: {payload['max_tokens']}")
        print(f"ğŸ“‹ æ¶ˆæ¯å†…å®¹é¢„è§ˆ:")
        for i, msg in enumerate(messages):
            print(f"  {i+1}. [{msg['role']}] {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}")
        
        # æ£€æŸ¥APIå¯†é’¥é•¿åº¦å’Œæ ¼å¼
        if len(self.glm_api_key) < 20:
            print(f"âš ï¸  è­¦å‘Šï¼šAPIå¯†é’¥é•¿åº¦å¼‚å¸¸ ({len(self.glm_api_key)} å­—ç¬¦)")
        
        headers = {
            "Authorization": f"Bearer {self.glm_api_key}",  # æ˜¾ç¤ºå®Œæ•´å¯†é’¥ä¾¿äºè°ƒè¯•
            "Content-Type": "application/json"
        }
        
        try:
            print("ğŸŒ æ­£åœ¨å‘é€è¯·æ±‚åˆ°GLM API...")
            print(f"ğŸ“¡ APIç«¯ç‚¹: {self.glm_base_url}")
            
            # è¯¦ç»†è®°å½•è¯·æ±‚ä½“
            print(f"ğŸ“¤ è¯·æ±‚ä½“é¢„è§ˆ:")
            print(f"  model: {payload['model']}")
            print(f"  temperature: {payload['temperature']}")
            print(f"  max_tokens: {payload['max_tokens']}")
            print(f"  stream: {payload['stream']}")
            
            response = requests.post(
                self.glm_base_url,
                json=payload,
                headers=headers,
                timeout=60  # 60ç§’è¶…æ—¶
            )
            
            print(f"ğŸ“¥ å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"ğŸ“¥ å“åº”å¤´: {dict(response.headers)}")
            
            # å¦‚æœæ˜¯401é”™è¯¯ï¼Œæ˜¾ç¤ºå“åº”å†…å®¹
            if response.status_code == 401:
                try:
                    error_response = response.json()
                    print(f"âŒ 401é”™è¯¯è¯¦æƒ…: {error_response}")
                except:
                    print(f"âŒ 401é”™è¯¯å“åº”å†…å®¹: {response.text[:500]}")
            
            response.raise_for_status()
            
            result = response.json()
            print("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            
            if 'choices' in result:
                print(f"ğŸ¯ è¿”å›é€‰æ‹©æ•°é‡: {len(result['choices'])}")
                if result['choices']:
                    content = result['choices'][0]['message']['content']
                    print(f"ğŸ“„ å“åº”å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    print(f"ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {content[:200]}{'...' if len(content) > 200 else ''}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ GLM API è°ƒç”¨å¤±è´¥: {e}")
            if hasattr(e, 'response'):
                print(f"ğŸ“ è¯·æ±‚è¯¦æƒ…: çŠ¶æ€ç  {e.response.status_code}")
                try:
                    error_content = e.response.json()
                    print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_content}")
                except:
                    print(f"ğŸ“ é”™è¯¯å“åº”: {e.response.text[:500]}")
            else:
                print(f"ğŸ“ è¯·æ±‚è¯¦æƒ…: æ— å“åº”å¯¹è±¡")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ GLM API å“åº”è§£æå¤±è´¥: {e}")
            return None
    
    def extract_paper_metadata(self, text: str) -> Dict[str, Any]:
        """
        ä»…ä½¿ç”¨ LLM æå–è®ºæ–‡ä¿¡æ¯ï¼›ä»»ä½•é”™è¯¯éƒ½ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸åšå…œåº•è§£æã€‚
        """
        print("=" * 60)
        print("å¼€å§‹è§£æè®ºæ–‡æ–‡æœ¬ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šæ— å…œåº•ï¼‰")
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print("=" * 60)

        # 1) å¿…é¡»æœ‰å¯ç”¨çš„ API Key
        if not self.glm_api_key or self.glm_api_key == "your_glm_api_key_here":
            raise RuntimeError("LLM ä¸å¯ç”¨ï¼šæœªé…ç½®æˆ–ä½¿ç”¨äº†å ä½ GLM_API_KEY")

        # 2) åªèµ° LLMï¼›å¤±è´¥å³æŠ›é”™
        try:
            result = self._extract_with_llm(text)
        except Exception as e:
            # é€ä¼ ä¸€å±‚ï¼Œç»™ä¸Šæ¸¸/HTTP å±‚è¿”å›æ¸…æ™°çš„ message
            raise RuntimeError(f"LLM è§£æå¤±è´¥: {e}")

        if not result:
            raise RuntimeError("LLM è¿”å›ç©ºç»“æœ")

        return result

    def _extract_with_llm(self, text: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMè§£ææ–‡æœ¬"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡è§£æåŠ©æ‰‹ã€‚è¯·ä»ç»™å®šçš„è®ºæ–‡æ–‡æœ¬ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

1. metadataï¼ˆå…ƒæ•°æ®ï¼‰:
   - title: è®ºæ–‡æ ‡é¢˜
   - authors: ä½œè€…åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"name": "ä½œè€…å§“å", "affiliation": "æ‰€å±æœºæ„"}]
   - year: å‘è¡¨å¹´ä»½
   - journal: æœŸåˆŠåç§°
   - articleType: æ–‡ç« ç±»å‹ï¼ˆå¦‚ï¼šjournal, conference, preprintï¼‰
   - doi: DOI
   - tags: ç›¸å…³æ ‡ç­¾

2. abstractï¼ˆæ‘˜è¦ï¼‰:
   - zh: ä¸­æ–‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
   - en: è‹±æ–‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰

3. keywordsï¼ˆå…³é”®è¯ï¼‰:
   - å…³é”®è¯åˆ—è¡¨

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¦‚æœæŸäº›ä¿¡æ¯æ— æ³•ä»æ–‡æœ¬ä¸­æå–ï¼Œè¯·è®¾ç½®ä¸ºnullæˆ–ç©ºæ•°ç»„ã€‚"""

        user_prompt = f"è¯·åˆ†æä»¥ä¸‹è®ºæ–‡æ–‡æœ¬ï¼Œæå–metadata, abstractå’Œkeywordsï¼š\n\n{text[:40000]}"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°è®¾ç½®
            response = self.call_llm(messages, temperature=0.6)  # ä½¿ç”¨0.6çš„æ¸©åº¦ï¼Œä¸ç¤ºä¾‹ä¸€è‡´
            
            if not response or 'choices' not in response:
                print("GLM å“åº”æ ¼å¼é”™è¯¯")
                return None
                
            content = response['choices'][0]['message']['content']
            
            # å°è¯•è§£æ JSON
            try:
                # æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åœ¨ä»£ç å—ä¸­ï¼‰
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                parsed_data = json.loads(content)
                return parsed_data
                
            except json.JSONDecodeError as e:
                print(f"GLM è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {e}")
                print(f"åŸå§‹å†…å®¹: {content}")
                return None
                
        except Exception as e:
            print(f"æå–è®ºæ–‡å…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_with_simple_parsing(self, text: str) -> Dict[str, Any]:
        """ç®€å•çš„æ–‡æœ¬è§£ææ–¹æ³•ï¼Œå½“LLMä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        import re
        
        print("ğŸ” å¼€å§‹ç®€å•æ–‡æœ¬è§£æ")
        print(f"ğŸ“ æ–‡æœ¬è¡Œæ•°: {len(text.split(chr(10)))}")
        
        lines = text.split('\n')
        
        # æ”¹è¿›çš„æ ‡é¢˜æå–é€»è¾‘
        title = "æœªå‘½åè®ºæ–‡"
        print(f"ğŸ” å¼€å§‹æå–æ ‡é¢˜ (æ£€æŸ¥å‰15è¡Œ)")
        for i, line in enumerate(lines[:15]):  # æ£€æŸ¥å‰15è¡Œ
            line = line.strip()
            # æ›´ä¸¥æ ¼çš„æ ‡é¢˜è¿‡æ»¤æ¡ä»¶
            if (len(line) > 10 and
                not line.startswith('#') and
                not line.startswith('http') and
                not any(keyword in line.lower() for keyword in ['abstract', 'introduction', 'keywords', 'doi:', 'author', 'authors', 'by', 'email', 'correspondence']) and
                # æ ‡é¢˜é€šå¸¸ä¸åŒ…å«å¤šä¸ªé€—å·åˆ†éš”çš„åå­—
                not re.search(r'\d+[,Ã—]?\d+[,Ã—]?\*?[,Ã—]?\$?\$?\{.*\}', line) and  # è¿‡æ»¤æ‰åŒ…å«ä¸Šæ ‡è„šæ³¨çš„è¡Œ
                # æ ‡é¢˜é€šå¸¸ä¸åŒ…å«é‚®ç®±æˆ–è”ç³»æ–¹å¼
                not '@' in line and not '.edu' in line and not '.org' in line):
                
                title = line
                print(f"âœ… æå–åˆ°æ ‡é¢˜: {title} (æ¥è‡ªç¬¬{i+1}è¡Œ)")
                break
        
        # æ”¹è¿›çš„ä½œè€…æå–é€»è¾‘
        authors = []
        print(f"ğŸ” å¼€å§‹æå–ä½œè€…ä¿¡æ¯")
        authors_found = False
        
        # åœ¨æ ‡é¢˜é™„è¿‘å¯»æ‰¾ä½œè€…ä¿¡æ¯
        title_line_index = -1
        for i, line in enumerate(lines[:15]):
            if line.strip() == title:
                title_line_index = i
                break
        
        if title_line_index >= 0:
            # æ£€æŸ¥æ ‡é¢˜åçš„å‡ è¡Œ
            for i in range(title_line_index + 1, min(title_line_index + 5, len(lines))):
                line = lines[i].strip()
                if line and not line.startswith('#'):
                    # æ£€æŸ¥æ˜¯å¦åƒä½œè€…è¡Œï¼ˆåŒ…å«å¤šä¸ªå§“åå’Œå¯èƒ½çš„æœºæ„ä¿¡æ¯ï¼‰
                    if (re.search(r'[A-Za-z][A-Za-z\s,.-]+\s[A-Za-z]', line) and
                        not 'abstract' in line.lower() and
                        not len(line) > 200):  # ä½œè€…è¡Œé€šå¸¸ä¸ä¼šå¤ªé•¿
                        
                        print(f"ğŸ“ åœ¨ç¬¬{i+1}è¡Œå‘ç°ä½œè€…ä¿¡æ¯: {line}")
                        
                        # ä½¿ç”¨æ›´ç²¾ç¡®çš„ä½œè€…ååˆ†å‰²
                        # å»é™¤å¸¸è§çš„æœºæ„ä¿¡æ¯
                        clean_line = re.sub(r'\([^)]*\)', '', line)  # å»é™¤æ‹¬å·å†…å®¹
                        clean_line = re.sub(r'\d+[,Ã—]?\d+[,Ã—]?\*?[,Ã—]?', '', clean_line)  # å»é™¤ä¸Šæ ‡
                        
                        # æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
                        author_parts = re.split(r'[,\s]+and\s+|\s+and\s+|,\s*|;\s*|;', clean_line)
                        for part in author_parts:
                            part = part.strip()
                            # è¿‡æ»¤æ‰æ˜æ˜¾çš„éå§“åå†…å®¹
                            if (part and
                                len(part) > 2 and
                                len(part) < 50 and
                                not re.match(r'^\d+$', part) and  # ä¸æ˜¯çº¯æ•°å­—
                                not '@' in part and  # ä¸æ˜¯é‚®ç®±
                                not any(keyword in part.lower() for keyword in ['university', 'institute', 'department', 'school', 'college'])):
                                authors.append({"name": part, "affiliation": ""})
                                print(f"  âœ… æå–ä½œè€…: {part}")
                        authors_found = True
                        break
        
        # æå–å¹´ä»½ - ä¼˜å…ˆåœ¨ç‰¹å®šä½ç½®æŸ¥æ‰¾
        year = None
        print(f"ğŸ” å¼€å§‹æå–å¹´ä»½")
        
        # 1. åœ¨æ ‡é¢˜é™„è¿‘æŸ¥æ‰¾å¹´ä»½
        for i in range(max(0, title_line_index - 3), min(len(lines), title_line_index + 10)):
            year_match = re.search(r'\b(19|20)\d{2}\b', lines[i])
            if year_match:
                year = int(year_match.group())
                print(f"ğŸ“… æå–åˆ°å¹´ä»½: {year} (æ¥è‡ªç¬¬{i+1}è¡Œ)")
                break
        
        # å¦‚æœä¸Šé¢æ²¡æ‰¾åˆ°ï¼Œæœç´¢æ•´ä¸ªæ–‡æœ¬
        if not year:
            year_match = re.search(r'\b(19|20)\d{2}\b', text)
            if year_match:
                year = int(year_match.group())
                print(f"ğŸ“… æå–åˆ°å¹´ä»½: {year} (å…¨æ–‡æœç´¢)")
        
        # æ”¹è¿›çš„æ‘˜è¦æå–é€»è¾‘
        abstract_text = ""
        abstract_found = False
        print(f"ğŸ” å¼€å§‹æå–æ‘˜è¦")
        
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            
            # å¯»æ‰¾æ‘˜è¦å¼€å§‹æ ‡è®°
            if (('abstract' in line_lower and ':' not in line_lower) or
                ('summary' in line_lower and ':' not in line_lower) or
                ('we present' in line_lower and len(line_lower) < 100)):
                
                abstract_found = True
                print(f"ğŸ“„ åœ¨ç¬¬{i+1}è¡Œå‘ç°æ‘˜è¦å¼€å§‹: {line.strip()}")
                
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦å¼€å§‹çœŸæ­£çš„æ‘˜è¦å†…å®¹
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if next_line and len(next_line) > 50:  # æ‘˜è¦é€šå¸¸æ¯”è¾ƒé•¿
                        abstract_text = next_line + " "
                        if len(abstract_text) < 300:  # å¦‚æœè¿˜ä¸å¤Ÿé•¿ï¼Œç»§ç»­è¯»å–
                            for j in range(i + 2, min(i + 10, len(lines))):
                                if lines[j].strip() and not lines[j].lower().startswith(('keywords', 'index terms', '1.', 'introduction')):
                                    abstract_text += lines[j] + " "
                                else:
                                    break
                        print(f"ğŸ“„ æ‘˜è¦æå–å†…å®¹: {abstract_text[:200]}...")
                break
        
        # æå–DOI
        doi = None
        print(f"ğŸ” å¼€å§‹æå–DOI")
        
        # åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æœç´¢DOI
        doi_patterns = [
            r'doi[:\s]*(10\.\d+[^\s\n]*)',
            r'DOI[:\s]*(10\.\d+[^\s\n]*)',
            r'10\.\d+[/\w\-.,;()\s]*',
        ]
        
        for pattern in doi_patterns:
            doi_match = re.search(pattern, text, re.IGNORECASE)
            if doi_match:
                doi = doi_match.group(1) if '10.' in doi_match.group() else doi_match.group()
                # æ¸…ç†DOI
                doi = re.sub(r'[.,;]+$', '', doi)  # å»é™¤æœ«å°¾çš„æ ‡ç‚¹
                print(f"ğŸ”— æå–åˆ°DOI: {doi}")
                break
        
        # æ”¹è¿›çš„å…³é”®è¯æå–é€»è¾‘
        keywords = []
        print(f"ğŸ” å¼€å§‹æå–å…³é”®è¯")
        
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            
            # å¯»æ‰¾å…³é”®è¯æ ‡è®°
            if (('keywords' in line_lower and ':' not in line_lower) or
                ('index terms' in line_lower) or
                (line_lower.startswith('keywords') or line_lower.startswith('index terms'))):
                
                print(f"ğŸ·ï¸  åœ¨ç¬¬{i+1}è¡Œå‘ç°å…³é”®è¯: {line.strip()}")
                
                # æå–å…³é”®è¯å†…å®¹
                keywords_text = line
                if ':' in keywords_text:
                    keywords_text = keywords_text.split(':', 1)[1]
                
                # æ¸…ç†å¹¶åˆ†å‰²å…³é”®è¯
                keywords_text = re.sub(r'[â€”â€“\-]+', ',', keywords_text)  # æ›¿æ¢å„ç§ç ´æŠ˜å·
                kw_list = re.split(r'[,;]+', keywords_text)
                
                extracted_keywords = []
                for kw in kw_list:
                    kw = kw.strip()
                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å…³é”®è¯çš„å†…å®¹
                    if (kw and
                        len(kw) > 2 and
                        len(kw) < 30 and
                        not re.match(r'^\d+$', kw) and  # ä¸æ˜¯çº¯æ•°å­—
                        not kw.lower().startswith(('and', 'or', 'the', 'a', 'an'))):
                        extracted_keywords.append(kw)
                
                keywords.extend(extracted_keywords)
                print(f"  ğŸ·ï¸  æå–å…³é”®è¯: {extracted_keywords}")
                break
        
        # æœŸåˆŠç±»å‹æ¨æ–­
        article_type = "journal"
        if any(keyword in text.lower() for keyword in ['conference', 'proceedings', 'workshop']):
            article_type = "conference"
        elif any(keyword in text.lower() for keyword in ['preprint', 'arxiv']):
            article_type = "preprint"
        
        result = {
            "metadata": {
                "title": title,
                "authors": authors,
                "year": year,
                "journal": "",
                "articleType": article_type,
                "doi": doi,
                "tags": []
            },
            "abstract": {
                "zh": None,  # ç®€å•è§£æä¸åŒºåˆ†è¯­è¨€
                "en": abstract_text.strip() if abstract_text.strip() else None
            },
            "keywords": keywords[:10]  # é™åˆ¶å…³é”®è¯æ•°é‡
        }
        
        print("âœ… ç®€å•æ–‡æœ¬è§£æå®Œæˆ")
        print(f"ğŸ“Š è§£æç»“æœæ‘˜è¦:")
        print(f"  ğŸ“ æ ‡é¢˜: {result['metadata']['title']}")
        print(f"  ğŸ‘¥ ä½œè€…æ•°é‡: {len(result['metadata']['authors'])}")
        if result['metadata']['authors']:
            print(f"  ğŸ‘¥ ä½œè€…åˆ—è¡¨: {[author['name'] for author in result['metadata']['authors']]}")
        print(f"  ğŸ“… å¹´ä»½: {result['metadata']['year']}")
        print(f"  ğŸ“„ æ‘˜è¦é•¿åº¦: {len(result['abstract']['en'] or '')} å­—ç¬¦")
        print(f"  ğŸ”— DOI: {result['metadata']['doi']}")
        print(f"  ğŸ·ï¸  å…³é”®è¯æ•°é‡: {len(result['keywords'])}")
        print(f"  ğŸ“ æ–‡ç« ç±»å‹: {result['metadata']['articleType']}")
        
        return result
    
    def simple_text_chat(self, user_message: str, system_message: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚") -> Optional[str]:
        """
        ç®€å•çš„æ–‡æœ¬å¯¹è¯æ¥å£
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            
        Returns:
            æ¨¡å‹å›å¤å†…å®¹æˆ– None
        """
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        response = self.call_llm(messages)
        
        if response and 'choices' in response and len(response['choices']) > 0:
            return response['choices'][0]['message']['content']
        
        return None

    def parse_text_to_blocks(self, text: str, section_context: str = "") -> List[Dict[str, Any]]:
        """
        è§£ææ–‡æœ¬å¹¶ç”Ÿæˆé€‚åˆæ·»åŠ åˆ°sectionä¸­çš„blockç»“æ„
        
        Args:
            text: éœ€è¦è§£æçš„æ–‡æœ¬å†…å®¹
            section_context: sectionçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¸®åŠ©å¤§æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œè§£æ
            
        Returns:
            è§£æåçš„blockåˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹è§£ææ–‡æœ¬ä¸ºblocks")
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"ğŸ“‹ Sectionä¸Šä¸‹æ–‡: {section_context}")
        
        if not self.glm_api_key or self.glm_api_key == "your_glm_api_key_here":
            print("âš ï¸ LLMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._simple_text_to_blocks(text)
        
        try:
            return self._extract_blocks_with_llm(text, section_context)
        except Exception as e:
            print(f"âŒ LLMè§£æå¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ç®€å•è§£æ")
            return self._simple_text_to_blocks(text)

    def _extract_blocks_with_llm(self, text: str, section_context: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMè§£ææ–‡æœ¬ä¸ºblocks"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·å°†ç»™å®šçš„æ–‡æœ¬å†…å®¹è§£æä¸ºç»“æ„åŒ–çš„blocksï¼Œæ¯ä¸ªblockä»£è¡¨ä¸€ä¸ªæ®µè½æˆ–å†…å®¹å•å…ƒã€‚

è¿”å›æ ¼å¼ä¸ºJSONæ•°ç»„ï¼Œæ¯ä¸ªblockå¯¹è±¡åŒ…å«ï¼š
1. type: blockç±»å‹ï¼Œå¯ä»¥æ˜¯ "paragraph"ï¼ˆæ®µè½ï¼‰, "list"ï¼ˆåˆ—è¡¨ï¼‰, "heading"ï¼ˆæ ‡é¢˜ï¼‰, "formula"ï¼ˆå…¬å¼ï¼‰, "table"ï¼ˆè¡¨æ ¼ï¼‰, "figure"ï¼ˆå›¾ç‰‡ï¼‰
2. content: å…·ä½“å†…å®¹
3. level: å¦‚æœæ˜¯headingç±»å‹ï¼Œè¡¨ç¤ºæ ‡é¢˜çº§åˆ«ï¼ˆ1-6ï¼‰
4. items: å¦‚æœæ˜¯listç±»å‹ï¼ŒåŒ…å«åˆ—è¡¨é¡¹æ•°ç»„

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚å¦‚æœæ˜¯å…¬å¼ï¼Œè¯·ä¿ç•™LaTeXæ ¼å¼ã€‚"""

        context_info = f"å½“å‰sectionä¸Šä¸‹æ–‡ï¼š{section_context}" if section_context else "æ— ç‰¹å®šsectionä¸Šä¸‹æ–‡"
        user_prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬è§£æä¸ºç»“æ„åŒ–çš„blocksï¼š

{context_info}

æ–‡æœ¬å†…å®¹ï¼š
{text[:30000]}"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response = self.call_llm(messages, temperature=0.3)
            
            if not response or 'choices' not in response:
                print("GLM å“åº”æ ¼å¼é”™è¯¯")
                return self._simple_text_to_blocks(text)
                
            content = response['choices'][0]['message']['content']
            
            # è§£æJSON
            try:
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                blocks = json.loads(content)
                
                # éªŒè¯blocksæ ¼å¼
                if isinstance(blocks, list):
                    validated_blocks = []
                    for block in blocks:
                        if isinstance(block, dict) and 'type' in block and 'content' in block:
                            # æ·»åŠ IDå’Œæ—¶é—´æˆ³
                            from ..utils.common import generate_id, get_current_time
                            block['id'] = generate_id()
                            block['createdAt'] = get_current_time()
                            validated_blocks.append(block)
                    
                    print(f"âœ… LLMè§£ææˆåŠŸï¼Œç”Ÿæˆ {len(validated_blocks)} ä¸ªblocks")
                    return validated_blocks
                else:
                    print("âŒ LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„blockæ•°ç»„")
                    return self._simple_text_to_blocks(text)
                    
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"åŸå§‹å†…å®¹: {content}")
                return self._simple_text_to_blocks(text)
                
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return self._simple_text_to_blocks(text)

    def _simple_text_to_blocks(self, text: str) -> List[Dict[str, Any]]:
        """ç®€å•çš„æ–‡æœ¬è§£æä¸ºblocksçš„æ–¹æ³•"""
        from ..utils.common import generate_id, get_current_time
        
        blocks = []
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for paragraph in paragraphs:
            # ç®€å•çš„ç±»å‹åˆ¤æ–­
            block_type = "paragraph"
            extra_props = {}
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜ï¼ˆä»¥#å¼€å¤´æˆ–è€…çŸ­ä¸”å¤§å†™è¾ƒå¤šï¼‰
            if paragraph.startswith('#'):
                block_type = "heading"
                level = len(paragraph) - len(paragraph.lstrip('#'))
                extra_props['level'] = min(level, 6)
                paragraph = paragraph.lstrip('#').strip()
            elif len(paragraph) < 100 and paragraph.isupper():
                block_type = "heading"
                extra_props['level'] = 2
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨
            elif any(paragraph.startswith(marker) for marker in ['- ', '* ', '+ ', '1. ', '2. ']):
                block_type = "list"
                items = [item.strip() for item in paragraph.split('\n') if item.strip()]
                extra_props['items'] = items
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¬å¼ï¼ˆç®€å•æ£€æµ‹ï¼‰
            elif '$' in paragraph or '\\' in paragraph:
                block_type = "formula"
            
            block = {
                'id': generate_id(),
                'type': block_type,
                'content': paragraph,
                'createdAt': get_current_time(),
                **extra_props
            }
            blocks.append(block)
        
        print(f"âœ… ç®€å•è§£æå®Œæˆï¼Œç”Ÿæˆ {len(blocks)} ä¸ªblocks")
        return blocks


# å…¨å±€å®ä¾‹
_llm_utils: Optional[LLMUtils] = None

def get_llm_utils() -> LLMUtils:
    """è·å– LLMUtils å…¨å±€å®ä¾‹"""
    global _llm_utils
    if _llm_utils is None:
        _llm_utils = LLMUtils()
    return _llm_utils