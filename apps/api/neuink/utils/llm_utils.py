"""
å¤§æ¨¡å‹å·¥å…·ç±»
æ”¯æŒå¤šç§å¤§æ¨¡å‹è°ƒç”¨ï¼Œç›®å‰é›†æˆ GLM-4.6 æ¨¡å‹
"""

import os
import sys
import json
import requests
import locale
from typing import Dict, Any, Optional, List
from enum import Enum

# è®¾ç½®ç¼–ç ä¸ºUTF-8ä»¥å¤„ç†Unicodeå­—ç¬¦
if sys.platform.startswith('win'):
    # åœ¨Windowsä¸‹è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç 
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())
    
    # è®¾ç½®é»˜è®¤ç¼–ç 
    if hasattr(sys, 'setdefaultencoding'):
        sys.setdefaultencoding('utf-8')
        
# å®‰å…¨çš„æ—¥å¿—æ‰“å°å‡½æ•°
def safe_print(*args, **kwargs):
    """å®‰å…¨çš„æ‰“å°å‡½æ•°ï¼Œé¿å…ç¼–ç é”™è¯¯"""
    try:
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # å¦‚æœä»æœ‰ç¼–ç é”™è¯¯ï¼Œå°è¯•ç”¨ASCIIç¼–ç 
        try:
            print(*[str(arg).encode('ascii', errors='ignore').decode('ascii') for arg in args], **kwargs)
        except:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè·³è¿‡è¿™äº›å‚æ•°
            safe_args = []
            for arg in args:
                try:
                    safe_args.append(str(arg).encode('ascii', errors='ignore').decode('ascii'))
                except:
                    safe_args.append("[encoding_error]")
            print(*safe_args, **kwargs)

class LLMModel(Enum):
    """æ”¯æŒçš„å¤§æ¨¡å‹æšä¸¾"""
    GLM_4_6 = "glm-4.6"
    GLM_4_5 = "glm-4.5"
    GLM_4_PLUS = "glm-4-plus"
    # æœªæ¥å¯ä»¥æ‰©å±•å…¶ä»–æ¨¡å‹
    # GPT_4 = "gpt-4"
    # CLAUDE_3 = "claude-3"

class LLMUtils:
    """å¤§æ¨¡å‹å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.glm_api_key = os.getenv('GLM_API_KEY')
        self.glm_base_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        print(f"GLM API Key çŠ¶æ€: {'[å·²é…ç½®]' if self.glm_api_key and self.glm_api_key != 'your_glm_api_key_here' else '[æœªé…ç½®æˆ–ä¸ºå ä½ç¬¦]'}")
        print(f"[APIç«¯ç‚¹]: {self.glm_base_url}")
        
    def call_llm(
        self,
        messages: List[Dict[str, str]],
        model: LLMModel = LLMModel.GLM_4_6,
        temperature: float = 1.0,
        max_tokens: int = 65536,
        stream: bool = False,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨å¤§æ¨¡å‹æ¥å£
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"role": "user", "content": "..."}]
            model: ä½¿ç”¨çš„æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            **kwargs: å…¶ä»–æ¨¡å‹ç‰¹å®šå‚æ•°
            
        Returns:
            æ¨¡å‹å“åº”ç»“æœæˆ– Noneï¼ˆå¦‚æœå‡ºé”™ï¼‰
        """
        if model == LLMModel.GLM_4_6:
            return self._call_glm(messages, temperature, max_tokens, stream, **kwargs)
        elif model == LLMModel.GLM_4_5:
            return self._call_glm(messages, temperature, max_tokens, stream, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
    
    def _call_glm(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
        stream: bool,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨ GLM æ¨¡å‹
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            GLM å“åº”ç»“æœæˆ– None
        """
        safe_print("å¼€å§‹è°ƒç”¨GLM API")
        safe_print(f"è¯·æ±‚æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        if not self.glm_api_key:
            safe_print("é”™è¯¯ï¼šæœªè®¾ç½® GLM_API_KEY ç¯å¢ƒå˜é‡")
            return None
            
        # ä½¿ç”¨æ›´æ–°çš„æ¨¡å‹åç§°
        payload = {
            "model": LLMModel.GLM_4_6.value,  # ä¿®å¤ï¼šä½¿ç”¨ glm-4.6
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
            **kwargs
        }
        
        safe_print(f"æ¨¡å‹: {payload['model']}")
        safe_print(f"æ¸©åº¦: {payload['temperature']}")
        safe_print(f"æœ€å¤§Tokenæ•°: {payload['max_tokens']}")
        safe_print("æ¶ˆæ¯å†…å®¹é¢„è§ˆ:")
        for i, msg in enumerate(messages):
            safe_print(f"  {i+1}. [{msg['role']}] {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}")
        
        # æ£€æŸ¥APIå¯†é’¥é•¿åº¦å’Œæ ¼å¼
        if len(self.glm_api_key) < 20:
            safe_print(f"è­¦å‘Šï¼šAPIå¯†é’¥é•¿åº¦å¼‚å¸¸ ({len(self.glm_api_key)} å­—ç¬¦)")
        
        headers = {
            "Authorization": f"Bearer {self.glm_api_key}",  # æ˜¾ç¤ºå®Œæ•´å¯†é’¥ä¾¿äºè°ƒè¯•
            "Content-Type": "application/json"
        }
        
        try:
            safe_print("æ­£åœ¨å‘é€è¯·æ±‚åˆ°GLM API...")
            safe_print(f"APIç«¯ç‚¹: {self.glm_base_url}")
            
            # è¯¦ç»†è®°å½•è¯·æ±‚ä½“
            safe_print("è¯·æ±‚ä½“é¢„è§ˆ:")
            safe_print(f"  model: {payload['model']}")
            safe_print(f"  temperature: {payload['temperature']}")
            safe_print(f"  max_tokens: {payload['max_tokens']}")
            safe_print(f"  stream: {payload['stream']}")
            
            response = requests.post(
                self.glm_base_url,
                json=payload,
                headers=headers,
                timeout=180  # 60ç§’è¶…æ—¶
            )
            
            safe_print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            safe_print(f"å“åº”å¤´: {dict(response.headers)}")
            
            # å¦‚æœæ˜¯401é”™è¯¯ï¼Œæ˜¾ç¤ºå“åº”å†…å®¹
            if response.status_code == 401:
                try:
                    error_response = response.json()
                    safe_print(f"401é”™è¯¯è¯¦æƒ…: {error_response}")
                except:
                    safe_print(f"401é”™è¯¯å“åº”å†…å®¹: {response.text[:500]}")
            
            response.raise_for_status()
            
            result = response.json()
            safe_print("GLM APIè°ƒç”¨æˆåŠŸ")
            
            if 'choices' in result:
                safe_print(f"è¿”å›é€‰æ‹©æ•°é‡: {len(result['choices'])}")
                if result['choices']:
                    content = result['choices'][0]['message']['content']
                    safe_print(f"å“åº”å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    safe_print(f"å“åº”å†…å®¹é¢„è§ˆ: {content[:200]}{'...' if len(content) > 200 else ''}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            safe_print(f"GLM API è°ƒç”¨å¤±è´¥: {e}")
            if hasattr(e, 'response'):
                safe_print(f"è¯·æ±‚è¯¦æƒ…: çŠ¶æ€ç  {e.response.status_code}")
                try:
                    error_content = e.response.json()
                    safe_print(f"é”™è¯¯è¯¦æƒ…: {error_content}")
                except:
                    safe_print(f"é”™è¯¯å“åº”: {e.response.text[:500]}")
            else:
                safe_print(f"è¯·æ±‚è¯¦æƒ…: æ— å“åº”å¯¹è±¡")
            return None
        except json.JSONDecodeError as e:
            safe_print(f"GLM API å“åº”è§£æå¤±è´¥: {e}")
            return None
    
    def extract_paper_metadata(self, text: str) -> Dict[str, Any]:
        """
        ä»…ä½¿ç”¨ LLM æå–è®ºæ–‡ä¿¡æ¯ï¼›ä»»ä½•é”™è¯¯éƒ½ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸åšå…œåº•è§£æã€‚
        """
        print("=" * 60)
        print("å¼€å§‹è§£æè®ºæ–‡æ–‡æœ¬ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šæ— å…œåº•ï¼‰")
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print("=" * 60)

        # 1) å¿…é¡»æœ‰å¯ç”¨çš„ API Key
        if not self.glm_api_key or self.glm_api_key == "your_glm_api_key_here":
            raise RuntimeError("LLM ä¸å¯ç”¨ï¼šæœªé…ç½®æˆ–ä½¿ç”¨äº†å ä½ GLM_API_KEY")

        # 2) åªèµ° LLMï¼›å¤±è´¥å³æŠ›é”™
        try:
            result = self._extract_with_llm(text)
        except Exception as e:
            # é€ä¼ ä¸€å±‚ï¼Œç»™ä¸Šæ¸¸/HTTP å±‚è¿”å›æ¸…æ™°çš„ message
            raise RuntimeError(f"LLM è§£æå¤±è´¥: {e}")

        if not result:
            raise RuntimeError("LLM è¿”å›ç©ºç»“æœ")

        return result

    def _extract_with_llm(self, text: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMè§£ææ–‡æœ¬"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡è§£æåŠ©æ‰‹ã€‚è¯·ä»ç»™å®šçš„è®ºæ–‡æ–‡æœ¬ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

1. metadataï¼ˆå…ƒæ•°æ®ï¼‰:
   - title: è®ºæ–‡æ ‡é¢˜
   - authors: ä½œè€…åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"name": "ä½œè€…å§“å", "affiliation": "æ‰€å±æœºæ„"}]
   - year: å‘è¡¨å¹´ä»½
   - journal: æœŸåˆŠåç§°
   - articleType: æ–‡ç« ç±»å‹ï¼ˆå¦‚ï¼šjournal, conference, preprintï¼‰
   - doi: DOI
   - tags: ç›¸å…³æ ‡ç­¾

2. abstractï¼ˆæ‘˜è¦ï¼‰:
   - zh: ä¸­æ–‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
   - en: è‹±æ–‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰

3. keywordsï¼ˆå…³é”®è¯ï¼‰:
   - å…³é”®è¯åˆ—è¡¨

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¦‚æœæŸäº›ä¿¡æ¯æ— æ³•ä»æ–‡æœ¬ä¸­æå–ï¼Œè¯·è®¾ç½®ä¸ºnullæˆ–ç©ºæ•°ç»„ã€‚"""

        user_prompt = f"è¯·åˆ†æä»¥ä¸‹è®ºæ–‡æ–‡æœ¬ï¼Œæå–metadata, abstractå’Œkeywordsï¼š\n\n{text[:40000]}"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°è®¾ç½®
            response = self.call_llm(messages, temperature=0.6)  # ä½¿ç”¨0.6çš„æ¸©åº¦ï¼Œä¸ç¤ºä¾‹ä¸€è‡´
            
            if not response or 'choices' not in response:
                print("GLM å“åº”æ ¼å¼é”™è¯¯")
                return None
                
            content = response['choices'][0]['message']['content']
            
            # å°è¯•è§£æ JSON
            try:
                # æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åœ¨ä»£ç å—ä¸­ï¼‰
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                parsed_data = json.loads(content)
                return parsed_data
                
            except json.JSONDecodeError as e:
                print(f"GLM è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {e}")
                print(f"åŸå§‹å†…å®¹: {content}")
                return None
                
        except Exception as e:
            print(f"æå–è®ºæ–‡å…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_with_simple_parsing(self, text: str) -> Dict[str, Any]:
        """ç®€å•çš„æ–‡æœ¬è§£ææ–¹æ³•ï¼Œå½“LLMä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        import re
        
        safe_print("å¼€å§‹ç®€å•æ–‡æœ¬è§£æ")
        safe_print(f"æ–‡æœ¬è¡Œæ•°: {len(text.split(chr(10)))}")
        
        lines = text.split('\n')
        
        # æ”¹è¿›çš„æ ‡é¢˜æå–é€»è¾‘
        title = "æœªå‘½åè®ºæ–‡"
        safe_print("å¼€å§‹æå–æ ‡é¢˜ (æ£€æŸ¥å‰15è¡Œ)")
        for i, line in enumerate(lines[:15]):  # æ£€æŸ¥å‰15è¡Œ
            line = line.strip()
            # æ›´ä¸¥æ ¼çš„æ ‡é¢˜è¿‡æ»¤æ¡ä»¶
            if (len(line) > 10 and
                not line.startswith('#') and
                not line.startswith('http') and
                not any(keyword in line.lower() for keyword in ['abstract', 'introduction', 'keywords', 'doi:', 'author', 'authors', 'by', 'email', 'correspondence']) and
                # æ ‡é¢˜é€šå¸¸ä¸åŒ…å«å¤šä¸ªé€—å·åˆ†éš”çš„åå­—
                not re.search(r'\d+[,Ã—]?\d+[,Ã—]?\*?[,Ã—]?\$?\$?\{.*\}', line) and  # è¿‡æ»¤æ‰åŒ…å«ä¸Šæ ‡è„šæ³¨çš„è¡Œ
                # æ ‡é¢˜é€šå¸¸ä¸åŒ…å«é‚®ç®±æˆ–è”ç³»æ–¹å¼
                not '@' in line and not '.edu' in line and not '.org' in line):
                
                title = line
                safe_print(f"æå–åˆ°æ ‡é¢˜: {title} (æ¥è‡ªç¬¬{i+1}è¡Œ)")
                break
        
        # æ”¹è¿›çš„ä½œè€…æå–é€»è¾‘
        authors = []
        safe_print("å¼€å§‹æå–ä½œè€…ä¿¡æ¯")
        authors_found = False
        
        # åœ¨æ ‡é¢˜é™„è¿‘å¯»æ‰¾ä½œè€…ä¿¡æ¯
        title_line_index = -1
        for i, line in enumerate(lines[:15]):
            if line.strip() == title:
                title_line_index = i
                break
        
        if title_line_index >= 0:
            # æ£€æŸ¥æ ‡é¢˜åçš„å‡ è¡Œ
            for i in range(title_line_index + 1, min(title_line_index + 5, len(lines))):
                line = lines[i].strip()
                if line and not line.startswith('#'):
                    # æ£€æŸ¥æ˜¯å¦åƒä½œè€…è¡Œï¼ˆåŒ…å«å¤šä¸ªå§“åå’Œå¯èƒ½çš„æœºæ„ä¿¡æ¯ï¼‰
                    if (re.search(r'[A-Za-z][A-Za-z\s,.-]+\s[A-Za-z]', line) and
                        not 'abstract' in line.lower() and
                        not len(line) > 200):  # ä½œè€…è¡Œé€šå¸¸ä¸ä¼šå¤ªé•¿
                        
                        safe_print(f"åœ¨ç¬¬{i+1}è¡Œå‘ç°ä½œè€…ä¿¡æ¯: {line}")
                        
                        # ä½¿ç”¨æ›´ç²¾ç¡®çš„ä½œè€…ååˆ†å‰²
                        # å»é™¤å¸¸è§çš„æœºæ„ä¿¡æ¯
                        clean_line = re.sub(r'\([^)]*\)', '', line)  # å»é™¤æ‹¬å·å†…å®¹
                        clean_line = re.sub(r'\d+[,Ã—]?\d+[,Ã—]?\*?[,Ã—]?', '', clean_line)  # å»é™¤ä¸Šæ ‡
                        
                        # æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
                        author_parts = re.split(r'[,\s]+and\s+|\s+and\s+|,\s*|;\s*|;', clean_line)
                        for part in author_parts:
                            part = part.strip()
                            # è¿‡æ»¤æ‰æ˜æ˜¾çš„éå§“åå†…å®¹
                            if (part and
                                len(part) > 2 and
                                len(part) < 50 and
                                not re.match(r'^\d+$', part) and  # ä¸æ˜¯çº¯æ•°å­—
                                not '@' in part and  # ä¸æ˜¯é‚®ç®±
                                not any(keyword in part.lower() for keyword in ['university', 'institute', 'department', 'school', 'college'])):
                                authors.append({"name": part, "affiliation": ""})
                                safe_print(f"  æå–ä½œè€…: {part}")
                        authors_found = True
                        break
        
        # æå–å¹´ä»½ - ä¼˜å…ˆåœ¨ç‰¹å®šä½ç½®æŸ¥æ‰¾
        year = None
        safe_print("å¼€å§‹æå–å¹´ä»½")
        
        # 1. åœ¨æ ‡é¢˜é™„è¿‘æŸ¥æ‰¾å¹´ä»½
        for i in range(max(0, title_line_index - 3), min(len(lines), title_line_index + 10)):
            year_match = re.search(r'\b(19|20)\d{2}\b', lines[i])
            if year_match:
                year = int(year_match.group())
                safe_print(f"æå–åˆ°å¹´ä»½: {year} (æ¥è‡ªç¬¬{i+1}è¡Œ)")
                break
        
        # å¦‚æœä¸Šé¢æ²¡æ‰¾åˆ°ï¼Œæœç´¢æ•´ä¸ªæ–‡æœ¬
        if not year:
            year_match = re.search(r'\b(19|20)\d{2}\b', text)
            if year_match:
                year = int(year_match.group())
                safe_print(f"æå–åˆ°å¹´ä»½: {year} (å…¨æ–‡æœç´¢)")
        
        # æ”¹è¿›çš„æ‘˜è¦æå–é€»è¾‘
        abstract_text = ""
        abstract_found = False
        safe_print("å¼€å§‹æå–æ‘˜è¦")
        
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            
            # å¯»æ‰¾æ‘˜è¦å¼€å§‹æ ‡è®°
            if (('abstract' in line_lower and ':' not in line_lower) or
                ('summary' in line_lower and ':' not in line_lower) or
                ('we present' in line_lower and len(line_lower) < 100)):
                
                abstract_found = True
                safe_print(f"åœ¨ç¬¬{i+1}è¡Œå‘ç°æ‘˜è¦å¼€å§‹: {line.strip()}")
                
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦å¼€å§‹çœŸæ­£çš„æ‘˜è¦å†…å®¹
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if next_line and len(next_line) > 50:  # æ‘˜è¦é€šå¸¸æ¯”è¾ƒé•¿
                        abstract_text = next_line + " "
                        if len(abstract_text) < 300:  # å¦‚æœè¿˜ä¸å¤Ÿé•¿ï¼Œç»§ç»­è¯»å–
                            for j in range(i + 2, min(i + 10, len(lines))):
                                if lines[j].strip() and not lines[j].lower().startswith(('keywords', 'index terms', '1.', 'introduction')):
                                    abstract_text += lines[j] + " "
                                else:
                                    break
                        safe_print(f"æ‘˜è¦æå–å†…å®¹: {abstract_text[:200]}...")
                break
        
        # æå–DOI
        doi = None
        safe_print("å¼€å§‹æå–DOI")
        
        # åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æœç´¢DOI
        doi_patterns = [
            r'doi[:\s]*(10\.\d+[^\s\n]*)',
            r'DOI[:\s]*(10\.\d+[^\s\n]*)',
            r'10\.\d+[/\w\-.,;()\s]*',
        ]
        
        for pattern in doi_patterns:
            doi_match = re.search(pattern, text, re.IGNORECASE)
            if doi_match:
                doi = doi_match.group(1) if '10.' in doi_match.group() else doi_match.group()
                # æ¸…ç†DOI
                doi = re.sub(r'[.,;]+$', '', doi)  # å»é™¤æœ«å°¾çš„æ ‡ç‚¹
                safe_print(f"æå–åˆ°DOI: {doi}")
                break
        
        # æ”¹è¿›çš„å…³é”®è¯æå–é€»è¾‘
        keywords = []
        safe_print("å¼€å§‹æå–å…³é”®è¯")
        
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            
            # å¯»æ‰¾å…³é”®è¯æ ‡è®°
            if (('keywords' in line_lower and ':' not in line_lower) or
                ('index terms' in line_lower) or
                (line_lower.startswith('keywords') or line_lower.startswith('index terms'))):
                
                safe_print(f"åœ¨ç¬¬{i+1}è¡Œå‘ç°å…³é”®è¯: {line.strip()}")
                
                # æå–å…³é”®è¯å†…å®¹
                keywords_text = line
                if ':' in keywords_text:
                    keywords_text = keywords_text.split(':', 1)[1]
                
                # æ¸…ç†å¹¶åˆ†å‰²å…³é”®è¯
                keywords_text = re.sub(r'[â€”â€“\-]+', ',', keywords_text)  # æ›¿æ¢å„ç§ç ´æŠ˜å·
                kw_list = re.split(r'[,;]+', keywords_text)
                
                extracted_keywords = []
                for kw in kw_list:
                    kw = kw.strip()
                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å…³é”®è¯çš„å†…å®¹
                    if (kw and
                        len(kw) > 2 and
                        len(kw) < 30 and
                        not re.match(r'^\d+$', kw) and  # ä¸æ˜¯çº¯æ•°å­—
                        not kw.lower().startswith(('and', 'or', 'the', 'a', 'an'))):
                        extracted_keywords.append(kw)
                
                keywords.extend(extracted_keywords)
                safe_print(f"  æå–å…³é”®è¯: {extracted_keywords}")
                break
        
        # æœŸåˆŠç±»å‹æ¨æ–­
        article_type = "journal"
        if any(keyword in text.lower() for keyword in ['conference', 'proceedings', 'workshop']):
            article_type = "conference"
        elif any(keyword in text.lower() for keyword in ['preprint', 'arxiv']):
            article_type = "preprint"
        
        result = {
            "metadata": {
                "title": title,
                "authors": authors,
                "year": year,
                "journal": "",
                "articleType": article_type,
                "doi": doi,
                "tags": []
            },
            "abstract": {
                "zh": None,  # ç®€å•è§£æä¸åŒºåˆ†è¯­è¨€
                "en": abstract_text.strip() if abstract_text.strip() else None
            },
            "keywords": keywords[:10]  # é™åˆ¶å…³é”®è¯æ•°é‡
        }
        
        safe_print("ç®€å•æ–‡æœ¬è§£æå®Œæˆ")
        safe_print("è§£æç»“æœæ‘˜è¦:")
        safe_print(f"  æ ‡é¢˜: {result['metadata']['title']}")
        safe_print(f"  ä½œè€…æ•°é‡: {len(result['metadata']['authors'])}")
        if result['metadata']['authors']:
            safe_print(f"  ä½œè€…åˆ—è¡¨: {[author['name'] for author in result['metadata']['authors']]}")
        safe_print(f"  å¹´ä»½: {result['metadata']['year']}")
        safe_print(f"  æ‘˜è¦é•¿åº¦: {len(result['abstract']['en'] or '')} å­—ç¬¦")
        safe_print(f"  DOI: {result['metadata']['doi']}")
        safe_print(f"  å…³é”®è¯æ•°é‡: {len(result['keywords'])}")
        safe_print(f"  æ–‡ç« ç±»å‹: {result['metadata']['articleType']}")
        
        return result
    
    def simple_text_chat(self, user_message: str, system_message: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚") -> Optional[str]:
        """
        ç®€å•çš„æ–‡æœ¬å¯¹è¯æ¥å£
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            
        Returns:
            æ¨¡å‹å›å¤å†…å®¹æˆ– None
        """
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        response = self.call_llm(messages)
        
        if response and 'choices' in response and len(response['choices']) > 0:
            return response['choices'][0]['message']['content']
        
        return None

    def parse_text_to_blocks(self, text: str, section_context: str = "") -> List[Dict[str, Any]]:
        """
        è§£ææ–‡æœ¬å¹¶ç”Ÿæˆé€‚åˆæ·»åŠ åˆ°sectionä¸­çš„blockç»“æ„
        
        Args:
            text: éœ€è¦è§£æçš„æ–‡æœ¬å†…å®¹
            section_context: sectionçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¸®åŠ©å¤§æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œè§£æ
            
        Returns:
            è§£æåçš„blockåˆ—è¡¨
        """
        safe_print("å¼€å§‹è§£ææ–‡æœ¬ä¸ºblocks")
        safe_print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        if not self.glm_api_key or self.glm_api_key == "your_glm_api_key_here":
            safe_print("LLMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._simple_text_to_blocks(text)
        
        try:
            return self._extract_blocks_with_llm(text, section_context)
        except Exception as e:
            safe_print(f"LLMè§£æå¤±è´¥: {e}")
            safe_print("å›é€€åˆ°ç®€å•è§£æ")
            return self._simple_text_to_blocks(text)

    def parse_text_to_blocks_and_save(
        self,
        text: str,
        paper_id: str,
        section_id: str,
        section_context: str = "",
        user_id: str = "",
        after_block_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è§£ææ–‡æœ¬ä¸ºblockså¹¶ç›´æ¥ä¿å­˜åˆ°è®ºæ–‡ä¸­
        
        Args:
            text: éœ€è¦è§£æçš„æ–‡æœ¬å†…å®¹
            paper_id: è®ºæ–‡ID
            section_id: section ID
            section_context: sectionçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            user_id: ç”¨æˆ·ID
            after_block_id: åœ¨æŒ‡å®šblockåæ’å…¥ï¼Œä¸ä¼ åˆ™åœ¨æœ«å°¾æ·»åŠ 
            
        Returns:
            ä¿å­˜ç»“æœ
        """
        try:
            safe_print("å¼€å§‹è§£ææ–‡æœ¬å¹¶ä¿å­˜åˆ°è®ºæ–‡")
            safe_print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            safe_print(f"è®ºæ–‡ID: {paper_id}")
            safe_print(f"Section ID: {section_id}")
            
            # é¦–å…ˆè§£ææ–‡æœ¬ä¸ºblocks
            parsed_blocks = self.parse_text_to_blocks(text, section_context)
            
            if not parsed_blocks:
                return {
                    "success": False,
                    "error": "æ–‡æœ¬è§£æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„blocks"
                }
            
            safe_print(f"æˆåŠŸè§£æç”Ÿæˆ {len(parsed_blocks)} ä¸ªblocks")
            
            # è·å–paperæœåŠ¡æ¥ä¿å­˜
            from ..services.paperService import get_paper_service
            paper_service = get_paper_service()
            
            # ç›´æ¥æ›´æ–°è®ºæ–‡ï¼Œä¸è°ƒç”¨å¯èƒ½ä¸å…¼å®¹çš„add_blocks_to_sectionæ–¹æ³•
            paper = paper_service.paper_model.find_by_id(paper_id)
            if paper:
                sections = paper.get("sections", [])
                target_section = None
                section_index = -1
                
                for i, section in enumerate(sections):
                    if section.get("id") == section_id:
                        target_section = section
                        section_index = i
                        break
                
                if target_section:
                    # ç¡®ä¿sectionæœ‰contentå­—æ®µ
                    if "content" not in target_section:
                        target_section["content"] = []
                    
                    # æ ¹æ®after_block_idç¡®å®šæ’å…¥ä½ç½®
                    current_blocks = target_section["content"]
                    insert_index = len(current_blocks)  # é»˜è®¤åœ¨æœ«å°¾
                    
                    if after_block_id:
                        for i, block in enumerate(current_blocks):
                            if block.get("id") == after_block_id:
                                insert_index = i + 1  # æ’å…¥åˆ°æŒ‡å®šblockåé¢
                                break
                    
                    # å®‰å…¨åœ°æ’å…¥æ–°blocks
                    safe_print(f"åœ¨ä½ç½® {insert_index} æ’å…¥ {len(parsed_blocks)} ä¸ªblocks")
                    new_blocks = current_blocks[:insert_index] + parsed_blocks + current_blocks[insert_index:]
                    target_section["content"] = new_blocks
                    sections[section_index] = target_section
                    
                    # æ›´æ–°è®ºæ–‡
                    update_data = {"sections": sections}
                    if paper_service.paper_model.update(paper_id, update_data):
                        updated_paper = paper_service.paper_model.find_by_id(paper_id)
                        return {
                            "success": True,
                            "message": f"æˆåŠŸå‘sectionæ·»åŠ äº†{len(parsed_blocks)}ä¸ªblocks",
                            "data": {
                                "paper": updated_paper,
                                "addedBlocks": parsed_blocks,
                                "sectionId": section_id,
                                "totalBlocks": len(parsed_blocks)
                            }
                        }
                    else:
                        return {
                            "success": False,
                            "error": "æ›´æ–°è®ºæ–‡å¤±è´¥"
                        }
                else:
                    return {
                        "success": False,
                        "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„section: {section_id}"
                    }
            else:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„è®ºæ–‡: {paper_id}"
                }
            
        except Exception as e:
            safe_print(f"è§£æå¹¶ä¿å­˜å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"è§£æå¹¶ä¿å­˜å¤±è´¥: {str(e)}"
            }

    def _ensure_bilingual_inline(self,inline_items):
        """å°† InlineContent æ•°ç»„ä¸­ç¼ºå¤±è¯­è¨€çš„æ–‡æœ¬å¤åˆ¶ä¸ºåŒå†…å®¹ï¼Œç¡®ä¿ en/zh éƒ½æœ‰å€¼"""
        if not isinstance(inline_items, list):
            return []
        normalized = []
        for item in inline_items:
            if isinstance(item, dict):
                normalized.append(item)
            else:
                normalized.append({"type": "text", "text": str(item)})
        return normalized

    def _fill_missing_languages(self,block):
        """åœ¨ block çš„ content ä¸­è¡¥å…¨ en/zhï¼Œè‹¥ç¼ºå¤±åˆ™ä»¥å¦ä¸€è¯­è¨€å†…å®¹æˆ–ç©ºæ•°ç»„å›å¡«"""
        content = block.get("content")
        if not isinstance(content, dict):
            return block
        en = self._ensure_bilingual_inline(content.get("en"))
        zh = self._ensure_bilingual_inline(content.get("zh"))
        if not en and zh:
            en = [dict(item) for item in zh]
        if not zh and en:
            zh = [dict(item) for item in en]
        block["content"] = {"en": en, "zh": zh}
        return block

    def _extract_blocks_with_llm(self, text: str, section_context: str) -> List[Dict[str, Any]]:
        import json
        import re

        PARSER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡å†…å®¹ç»“æ„åŒ–åŠ©æ‰‹ï¼Œä¸“æ³¨äºå°†æ–‡æœ¬å†…å®¹è§£æä¸ºæ ‡å‡†åŒ–çš„blockæ•°ç»„ã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼š**
å°†è¾“å…¥çš„æ–‡æœ¬å†…å®¹è§£æä¸ºç¬¦åˆå­¦æœ¯è®ºæ–‡ç¼–è¾‘ç³»ç»Ÿçš„blockç»“æ„ï¼Œæ¯ä¸ªblockå¿…é¡»åŒ…å«ä¸­è‹±æ–‡å†…å®¹ã€‚

**ä¸¥æ ¼è¾“å‡ºè¦æ±‚ï¼š**
1. **å¿…é¡»è¾“å‡ºçº¯JSONæ•°ç»„æ ¼å¼** - ä»¥[å¼€å¤´ï¼Œ]ç»“å°¾ï¼Œä¸å…è®¸ä»»ä½•å…¶ä»–æ–‡å­—æˆ–æ ¼å¼
2. **æ¯ä¸ªblockçš„contentå¿…é¡»åŒæ—¶åŒ…å«enå’Œzhä¸¤ä¸ªè¯­è¨€æ•°ç»„**
3. **å³ä½¿åŸå§‹æ–‡æœ¬æ˜¯çº¯ä¸­æ–‡ï¼Œä¹Ÿè¦ç”Ÿæˆå¯¹åº”çš„è‹±æ–‡ç¿»è¯‘ï¼›åä¹‹äº¦ç„¶**
4. **ä¸å…è®¸ä»»ä½•å­—æ®µç¼ºå¤±æˆ–ä¸ºç©º**

**Blockç±»å‹åŠå­—æ®µè§„èŒƒï¼š**
- heading: æ ‡é¢˜ (éœ€è¦levelå­—æ®µ1-6)
- paragraph: æ®µè½ (content: {en: [...], zh: [...]})
- ordered-list: æœ‰åºåˆ—è¡¨ (itemsæ•°ç»„ï¼Œæ¯é¡¹åŒ…å«contentåŒè¯­è¨€)
- unordered-list: æ— åºåˆ—è¡¨ (itemsæ•°ç»„ï¼Œæ¯é¡¹åŒ…å«contentåŒè¯­è¨€)
- quote: å¼•ç”¨ (éœ€è¦authorå­—æ®µï¼ŒcontentåŒè¯­è¨€)
- math: æ•°å­¦å…¬å¼ (latexå­—æ®µä¿ç•™åŸå§‹å…¬å¼ï¼Œcontentä¸ºè§£é‡Šæ–‡å­—)
- code: ä»£ç  (languageå­—æ®µï¼Œcodeå­—æ®µä¿ç•™åŸå§‹ä»£ç )
- figure: å›¾ç‰‡ (src, alt, captionåŒè¯­è¨€)
- table: è¡¨æ ¼ (headers, rows, align)
- divider: åˆ†å‰²çº¿

**InlineContentç±»å‹ï¼š**
- text: æ™®é€šæ–‡æœ¬ï¼Œå¯åŒ…å«æ ·å¼
- link: é“¾æ¥
- inline-math: è¡Œå†…æ•°å­¦å…¬å¼
- citation: å¼•ç”¨

**ç¿»è¯‘è¦æ±‚ï¼š**
- å¦‚æœåŸå§‹å†…å®¹æ˜¯ä¸­æ–‡ï¼Œzhæ•°ç»„æ”¾åŸæ–‡ï¼Œenæ•°ç»„æ”¾å‡†ç¡®è‹±æ–‡ç¿»è¯‘
- å¦‚æœåŸå§‹å†…å®¹æ˜¯è‹±æ–‡ï¼Œenæ•°ç»„æ”¾åŸæ–‡ï¼Œzhæ•°ç»„æ”¾å‡†ç¡®ä¸­æ–‡ç¿»è¯‘
- ä¿æŒå­¦æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
- ä¿æŒæ•°å­¦å…¬å¼ã€ä»£ç ã€å¼•ç”¨ç­‰ç‰¹æ®Šå†…å®¹çš„æ ¼å¼

**é‡è¦æé†’ï¼š**
- æ¯ä¸ªblockçš„content.enå’Œcontent.zhéƒ½å¿…é¡»æ˜¯æ•°ç»„ï¼Œä¸èƒ½ä¸ºç©º
- å¦‚æœæ— æ³•ç¿»è¯‘ï¼Œå¤åˆ¶åŸæ–‡åˆ°ç›®æ ‡è¯­è¨€æ•°ç»„
- ä¸¥æ ¼éµå¾ªJSONæ ¼å¼ï¼Œä¸èƒ½æœ‰æ³¨é‡Šæˆ–é¢å¤–æ–‡å­—"""

        TRANSLATION_SYSTEM_PROMPT = """You are a professional academic translator specializing in scholarly content.

**Translation Requirements:**
- Preserve academic terminology and precision
- Maintain mathematical formulas, code, and technical terms exactly as they appear
- For Chinese to English: Use formal academic English appropriate for research papers
- For English to Chinese: Use standard academic Chinese terminology
- Keep citations, references, and special formatting intact
- Output ONLY the translated text without any additional explanation or formatting markers"""

        chinese_char = re.compile(r"[\u4e00-\u9fff]")

        def _parse_markdown():
            safe_print("ğŸš€ å¼€å§‹ä½¿ç”¨LLMè§£ææ–‡æœ¬")
            section_title = section_context.splitlines()[0].strip() if section_context else ""
            context_info = f"å½“å‰sectionæ ‡é¢˜ï¼š{section_title}" if section_title else "æ— ç‰¹å®šsectionæ ‡é¢˜"
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥é¿å…è¶…å‡ºtokené™åˆ¶
            truncated_text = text[:40000] if len(text) > 40000 else text
            safe_print(f"ğŸ” åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            safe_print(f"ğŸ“ è§£ææ–‡æœ¬é•¿åº¦: {len(truncated_text)} å­—ç¬¦")
            if len(text) > 40000:
                safe_print("âš ï¸  æ–‡æœ¬è¢«æˆªæ–­ï¼Œå‰40,000å­—ç¬¦å°†è¢«è§£æ")
                safe_print(f"ğŸ“‹ æˆªæ–­çš„æ–‡æœ¬é¢„è§ˆ: {truncated_text[:200]}...")
            safe_print(f"ğŸ“Š æ–‡æœ¬å†…å®¹ç»Ÿè®¡: æ€»è¡Œæ•°={len(text.split(chr(10)))}")
            
            user_prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬å†…å®¹è§£æä¸ºæ ‡å‡†åŒ–çš„blockæ•°ç»„ã€‚

{context_info}

éœ€è¦è§£æçš„æ–‡æœ¬å†…å®¹ï¼š
{truncated_text}

**é‡è¦è¦æ±‚ï¼š**
1. æ¯ä¸ªblockçš„contentå¿…é¡»åŒæ—¶åŒ…å«enå’Œzhä¸¤ä¸ªè¯­è¨€æ•°ç»„
2. å¦‚æœåŸæ–‡æœ¬æ˜¯ä¸­æ–‡ï¼Œzhæ”¾åŸæ–‡ï¼Œenæ”¾è‹±æ–‡ç¿»è¯‘
3. å¦‚æœåŸæ–‡æœ¬æ˜¯è‹±æ–‡ï¼Œenæ”¾åŸæ–‡ï¼Œzhæ”¾ä¸­æ–‡ç¿»è¯‘
4. ä¿æŒå­¦æœ¯å†…å®¹çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
5. ä¸¥æ ¼è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦å…¶ä»–ä»»ä½•æ–‡å­—"""

            messages = [
                {"role": "system", "content": PARSER_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ]
            
            safe_print("ğŸ“¤ å‘é€è§£æè¯·æ±‚åˆ°LLM...")
            response = self.call_llm(messages, temperature=0.2, max_tokens=8000)
            
            if not response or "choices" not in response:
                safe_print("âŒ LLMå“åº”æ ¼å¼é”™è¯¯")
                raise Exception("LLMå“åº”æ ¼å¼é”™è¯¯")
                
            content = response["choices"][0]["message"]["content"]
            safe_print(f"ğŸ’¬ LLMåŸå§‹å“åº”: {content[:500]}...")
            
            # æå–JSONå†…å®¹
            if "```json" in content:
                fence = "```json"
                start = content.find(fence) + len(fence)
                end = content.find("```", start)
                content = content[start:end].strip()
            elif "```" in content:
                fence = "```"
                start = content.find(fence) + len(fence)
                end = content.find("```", start)
                content = content[start:end].strip()
            
            # æ¸…ç†å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
            content = content.strip()
            if content.startswith('json'):
                content = content[4:].strip()
            
            safe_print(f"ğŸ”„ è§£æJSONå†…å®¹: {content[:200]}...")
            return json.loads(content)

        def _normalize_inline(items):
            """æ ‡å‡†åŒ–InlineContentæ•°ç»„"""
            normalized = []
            for item in items or []:
                if isinstance(item, dict):
                    normalized.append(item)
                elif isinstance(item, str):
                    normalized.append({"type": "text", "text": item})
            return normalized

        def _inline_plain_text(items):
            """æå–çº¯æ–‡æœ¬å†…å®¹ç”¨äºç¿»è¯‘"""
            fragments = []
            for item in items or []:
                if not isinstance(item, dict):
                    continue
                if item.get("type") == "text" and item.get("text"):
                    fragments.append(item["text"])
                elif item.get("type") == "link":
                    label = item.get("label") or item.get("text")
                    if label:
                        fragments.append(label)
            return " ".join(fragments).strip()

        def _rebuild_inline_from_text(text_value):
            """ä»çº¯æ–‡æœ¬é‡å»ºInlineContent"""
            text_value = (text_value or "").strip()
            return [{"type": "text", "text": text_value}] if text_value else []

        def _translate_content(source_items, target_lang):
            """ç¿»è¯‘InlineContentå†…å®¹"""
            plain_text = _inline_plain_text(source_items)
            if not plain_text:
                return []
            
            source_lang = "zh" if chinese_char.search(plain_text) else "en"
            if source_lang == target_lang:
                # åŒè¯­è¨€ï¼Œç›´æ¥è¿”å›
                return [dict(item) for item in source_items]
            
            messages = [
                {"role": "system", "content": TRANSLATION_SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"Source language: {source_lang}\nTarget language: {target_lang}\n\nSource text:\n{plain_text}\n\nReturn only the translated text.",
                },
            ]
            
            try:
                safe_print(f"ğŸ”„ ç¿»è¯‘å†…å®¹ ({source_lang} -> {target_lang}): {plain_text[:50]}...")
                resp = self.call_llm(messages, temperature=0.1, max_tokens=2000)
                
                if resp and "choices" in resp:
                    translated = resp["choices"][0]["message"]["content"].strip()
                    safe_print(f"âœ… ç¿»è¯‘å®Œæˆ: {translated[:50]}...")
                    return _rebuild_inline_from_text(translated)
                else:
                    safe_print("âŒ ç¿»è¯‘å“åº”æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨åŸæ–‡")
                    return [dict(item) for item in source_items]
            except Exception as exc:
                safe_print(f"âŒ ç¿»è¯‘å¤±è´¥ ({source_lang}->{target_lang}): {exc}")
                return [dict(item) for item in source_items]

        def _ensure_bilingual_content(block):
            """ç¡®ä¿blockçš„contentåŒ…å«åŒè¯­è¨€å†…å®¹"""
            content = block.get("content")
            if not isinstance(content, dict):
                return block

            # è·å–ç°æœ‰å†…å®¹
            en_items = _normalize_inline(content.get("en", []))
            zh_items = _normalize_inline(content.get("zh", []))

            # å¦‚æœç¼ºå°‘æŸç§è¯­è¨€ï¼Œè¿›è¡Œç¿»è¯‘
            if not en_items and zh_items:
                en_items = _translate_content(zh_items, "en")
            if not zh_items and en_items:
                zh_items = _translate_content(en_items, "zh")

            # ç¡®ä¿ä¸¤ç§è¯­è¨€éƒ½æœ‰å†…å®¹
            if not en_items:
                en_items = [dict(item) for item in zh_items] if zh_items else []
            if not zh_items:
                zh_items = [dict(item) for item in en_items] if en_items else []

            block["content"] = {"en": en_items, "zh": zh_items}
            return block

        try:
            safe_print("ğŸ”„ å¼€å§‹LLMè§£ææµç¨‹")
            blocks = _parse_markdown()
        except Exception as exc:
            safe_print(f"âŒ LLMè§£æå¤±è´¥: {exc}")
            safe_print("ğŸ”™ å›é€€åˆ°ç®€å•è§£æ")
            return self._simple_text_to_blocks(text)

        if not isinstance(blocks, list):
            safe_print("âŒ LLMè¿”å›çš„ä¸æ˜¯blockåˆ—è¡¨")
            return self._simple_text_to_blocks(text)

        safe_print(f"âœ… éªŒè¯å’Œæ ‡å‡†åŒ–{len(blocks)}ä¸ªblocks...")
        from ..utils.common import generate_id, get_current_time

        validated_blocks = []
        for i, block in enumerate(blocks):
            try:
                if not isinstance(block, dict) or "type" not in block:
                    safe_print(f"â­ï¸  è·³è¿‡æ— æ•ˆblock: {block}")
                    continue

                # ç¡®ä¿contentæ˜¯å­—å…¸
                if "content" not in block or not isinstance(block["content"], dict):
                    block["content"] = {"en": [], "zh": []}
                
                # ç¡®ä¿åŒè¯­è¨€å†…å®¹
                block = _ensure_bilingual_content(block)
                
                # æ·»åŠ å¿…è¦å­—æ®µ
                block["id"] = generate_id()
                block["createdAt"] = get_current_time()
                
                validated_blocks.append(block)
                safe_print(f"âœ… éªŒè¯block {i+1}: {block.get('type')}")
                
            except Exception as exc:
                safe_print(f"âŒ éªŒè¯blockå¤±è´¥: {exc}")
                continue

        safe_print(f"ğŸ‰ å®ŒæˆéªŒè¯ï¼Œç”Ÿæˆ{len(validated_blocks)}ä¸ªæœ‰æ•ˆblocks")
        return validated_blocks

    def _simple_text_to_blocks(self, text: str) -> List[Dict[str, Any]]:
        """ç®€å•çš„æ–‡æœ¬è§£æä¸ºblocksçš„æ–¹æ³•"""
        from ..utils.common import generate_id, get_current_time
        import re
        
        safe_print("ğŸ”„ å¼€å§‹ç®€å•æ–‡æœ¬è§£æ")
        safe_print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        blocks = []
        lines = text.split('\n')
        
        # é¢„å¤„ç†ï¼šåˆå¹¶è¿ç»­çš„æ®µè½
        paragraphs = []
        current_para = []
        
        for line in lines:
            line = line.strip()
            if not line:
                if current_para:
                    paragraphs.append('\n'.join(current_para))
                    current_para = []
                continue
            
            # å¦‚æœæ˜¯æ ‡é¢˜è¡Œï¼Œå•ç‹¬æˆæ®µ
            if line.startswith('#') or (len(line) < 100 and line.isupper()):
                if current_para:
                    paragraphs.append('\n'.join(current_para))
                    current_para = []
                paragraphs.append(line)
            else:
                current_para.append(line)
        
        # æ·»åŠ æœ€åä¸€æ®µ
        if current_para:
            paragraphs.append('\n'.join(current_para))
        
        # è¿‡æ»¤ç©ºæ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        safe_print(f"ğŸ“Š è¯†åˆ«åˆ° {len(paragraphs)} ä¸ªæ®µè½")
        
        chinese_char = re.compile(r"[\u4e00-\u9fff]")
        
        for i, paragraph in enumerate(paragraphs):
            try:
                # åŸºæœ¬çš„ç±»å‹åˆ¤æ–­
                block_type = "paragraph"
                extra_props = {}
                content_text = paragraph
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
                if paragraph.startswith('#'):
                    block_type = "heading"
                    level = len(paragraph) - len(paragraph.lstrip('#'))
                    extra_props['level'] = min(level, 6)
                    content_text = paragraph.lstrip('#').strip()
                elif len(paragraph) < 100 and paragraph.isupper() and not re.search(r'[\.\!\?]', paragraph):
                    block_type = "heading"
                    extra_props['level'] = 2
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨
                elif any(paragraph.startswith(marker) for marker in ['- ', '* ', '+ ', '1. ', '2. ']):
                    block_type = "unordered-list"
                    items = []
                    for line in paragraph.split('\n'):
                        line = line.strip()
                        if line and line[0] in '-*+123456789':
                            # ç§»é™¤åˆ—è¡¨æ ‡è®°ç¬¦
                            clean_line = re.sub(r'^[\s\-*+\d\.]+', '', line).strip()
                            if clean_line:
                                # æ£€æµ‹è¯­è¨€å¹¶åˆ›å»ºåŒè¯­è¨€å†…å®¹
                                source_lang = "zh" if chinese_char.search(clean_line) else "en"
                                if source_lang == "zh":
                                    items.append({
                                        "content": {
                                            "zh": [{"type": "text", "text": clean_line}],
                                            "en": [{"type": "text", "text": clean_line}]  # ç®€å•å¤åˆ¶ï¼Œç­‰å¾…åç»­ç¿»è¯‘
                                        }
                                    })
                                else:
                                    items.append({
                                        "content": {
                                            "en": [{"type": "text", "text": clean_line}],
                                            "zh": [{"type": "text", "text": clean_line}]  # ç®€å•å¤åˆ¶ï¼Œç­‰å¾…åç»­ç¿»è¯‘
                                        }
                                    })
                    extra_props['items'] = items
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¬å¼
                elif '$' in paragraph or '$$' in paragraph or '\\' in paragraph:
                    block_type = "math"
                    # æå–LaTeXå…¬å¼
                    latex_matches = re.findall(r'\$\$?(.*?)\$?\$', paragraph, re.DOTALL)
                    if latex_matches:
                        extra_props['latex'] = latex_matches[0].strip()
                
                # åˆ›å»ºblock
                if block_type != "unordered-list":  # åˆ—è¡¨å·²ç»å¤„ç†äº†content
                    # æ£€æµ‹æ–‡æœ¬è¯­è¨€
                    source_lang = "zh" if chinese_char.search(content_text) else "en"
                    
                    if source_lang == "zh":
                        content = {
                            "zh": [{"type": "text", "text": content_text}],
                            "en": [{"type": "text", "text": content_text}]  # ç®€å•å¤åˆ¶ï¼Œç­‰å¾…åç»­ç¿»è¯‘
                        }
                    else:
                        content = {
                            "en": [{"type": "text", "text": content_text}],
                            "zh": [{"type": "text", "text": content_text}]  # ç®€å•å¤åˆ¶ï¼Œç­‰å¾…åç»­ç¿»è¯‘
                        }
                else:
                    content = {"en": [], "zh": []}  # åˆ—è¡¨çš„contentä¸ºç©ºï¼ŒitemsåŒ…å«å®é™…å†…å®¹
                
                block = {
                    'id': generate_id(),
                    'type': block_type,
                    'content': content,
                    'createdAt': get_current_time(),
                    **extra_props
                }
                
                # å¯¹äºç‰¹æ®Šç±»å‹æ·»åŠ é¢å¤–å±æ€§
                if block_type == "heading" and not extra_props.get('level'):
                    extra_props['level'] = 2
                
                blocks.append(block)
                safe_print(f"ğŸ“ ç”Ÿæˆblock {i+1}: {block_type}")
                
            except Exception as e:
                safe_print(f"âŒ å¤„ç†æ®µè½å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„paragraph block
                source_lang = "zh" if chinese_char.search(paragraph) else "en"
                if source_lang == "zh":
                    content = {
                        "zh": [{"type": "text", "text": paragraph}],
                        "en": [{"type": "text", "text": paragraph}]
                    }
                else:
                    content = {
                        "en": [{"type": "text", "text": paragraph}],
                        "zh": [{"type": "text", "text": paragraph}]
                    }
                
                block = {
                    'id': generate_id(),
                    'type': "paragraph",
                    'content': content,
                    'createdAt': get_current_time()
                }
                blocks.append(block)
        
        safe_print(f"ğŸ‰ ç®€å•è§£æå®Œæˆï¼Œç”Ÿæˆ {len(blocks)} ä¸ªblocks")
        return blocks


# å…¨å±€å®ä¾‹
_llm_utils: Optional[LLMUtils] = None

def get_llm_utils() -> LLMUtils:
    """è·å– LLMUtils å…¨å±€å®ä¾‹"""
    global _llm_utils
    if _llm_utils is None:
        _llm_utils = LLMUtils()
    return _llm_utils